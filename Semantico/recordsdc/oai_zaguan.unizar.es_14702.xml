<?xml version="1.0" encoding="UTF-8"?><oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
  <dc:creator>Bello Gimeno, Carlos</dc:creator>
  <dc:creator>Civera Sancho, Javier</dc:creator>
  <dc:title>Reconocimiento de acciones en vídeos de cámaras vestibles</dc:title>
  <dc:identifier>http://zaguan.unizar.es/record/14702</dc:identifier>
  <dc:publisher>Universidad de Zaragoza</dc:publisher>
  <dc:date>2014</dc:date>
  <dc:description>El reconocimiento de acciones es el problema consistente en clasificar de manera automática secuencias de imágenes en función de las acciones (beber, caminar, abrir ventana, leer, etc.) que contengan. La mayor parte del trabajo previo se ha realizado en vídeos tomados por terceras personas, provenientes de películas o de repositorios como YouTube. La contribución de este proyecto es el uso de secuencias de vídeo de cámaras vestibles. El interés por dichas cámaras es creciente debido a la aparición de diversos modelos comerciales (Google Glass, GoPro o Memoto). Todo proceso de clasificación tiene dos etapas básicas: entrenamiento y test. En ambas etapas cada vídeo se codifica mediante una serie de descriptores. En la etapa de entrenamiento se calcula el clasificador a partir de vídeos de entrenamiento, de los cuales conocemos la categoría. En la etapa de test el clasificador asigna a cada vídeo de test, de los cuales se desconoce la acción que contiene, una categoría en función de sus descriptores. El descriptor utilizado en este proyecto es el denominado bolsa de palabras. Dicho descriptor se calcula a partir de los puntos de interés espacio temporales (STIP). La bolsa de descriptores se construye mediante un proceso de clustering; el cual trata de agrupar los descriptores en grupos o clusters según su semejanza empleando una determinada distancia. En el proyecto se evalúa la influencia en los resultados del número de palabras y del número y valores de los descriptores elegidos para la construcción del vocabulario, así como de la distancia elegida. Para la clasificación de la acción realizada en el vídeo se utiliza un algoritmo de entrenamiento supervisado como es Support Vector Machine (SVM). Se estudian los resultados obtenidos en función del mayor o menor ajuste a la hora de trazar las fronteras entre las diferentes categorías de acciones. Dichos resultados son medidos por la precisión media de reconocimiento. El proyecto cuenta con un dataset propio de acciones grabadas con cámaras vestibles. Está formado por veinte acciones desarrolladas en dos escenarios diferentes (edificio Ada Byron y edificio I3A) por cinco usuarios diferentes. Los vídeos consisten en una secuencia de varias acciones consecutivas y han debido de ser manualmente etiquetados para su utilización. Cada vídeo ha sido grabado por cuatro cámaras vestibles diferentes: una cámara GoPro, un teléfono móvil, una cámara omnidireccional y una cámara de profundidad. El objetivo es identificar cuál de estos tipos de cámaras proporciona unos mejores resultados en la identificación de acciones en cámaras vestibles</dc:description>
  <dc:format>pdf</dc:format>
  <dc:language>spa</dc:language>
  <dc:type>info:eu-repo/semantics/masterThesis</dc:type>
  <dc:rights>by-nc-sa</dc:rights>
  <dc:rights>http://creativecommons.org/licenses/by-nc-sa/3.0/</dc:rights>
</oai_dc:dc>