<?xml version="1.0" encoding="UTF-8"?><oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
  <dc:creator>Morera Munt, Alba</dc:creator>
  <dc:creator>Alcalá Nalvaiz, José Tomás</dc:creator>
  <dc:title>Introducción a los modelos de redes neuronales artificiales. El Perceptrón simple y multicapa</dc:title>
  <dc:identifier>http://zaguan.unizar.es/record/69205</dc:identifier>
  <dc:publisher>Universidad de Zaragoza</dc:publisher>
  <dc:date>2018</dc:date>
  <dc:description>En los últimos años se ha consolidado un nuevo campo dentro de las ciencias de la computación que abarcaría un conjunto de metodologías que se caracterizan por su inspiración en los sistemas biológicos para resolver problemas relacionados con el mundo real; reconocimiento de imágenes y de voz, toma de decisiones, predicciones del tiempo atmosférico, etc. Las Redes Neuronales Artificiales, RNA, son las que actualmente están causando un mayor impacto, debido a su gran aplicación práctica, lo que ha llevado a incorporarlas al conjunto de herramientas estadísticas orientadas a la clasificación de patrones y la estimación de variables continuas. Estas redes son sistemas de procesamiento de la información cuya estructura y funcionamiento están inspirados en las redes neuronales biológicas. Consisten en un conjunto de elementos simples de procesamiento llamados neuronas conectadas entre sí por conexiones que tienen un valor numérico modificable llamado peso. La actividad que una neurona artificial realiza consiste en sumar los valores de las entradas que recibe, comparar esta cantidad con el valor umbral y, si lo iguala o supera, enviar una salida activada o en caso contrario, desactivada. Tanto las entradas que la unidad recibe como las salidas que envía dependen a su vez del peso de las conexiones por las cuales se realizan estas operaciones.  El objetivo de estas redes neuronales es calcular los pesos, que son lo parámetros del modelo y para ello vamos a ver distintos tipos de algoritmos de aprendizaje. En el capítulo 1, hacemos una introducción a las redes neuronales. Explicamos los tres tipos de neuronas artificiales que existen dependiendo de la capa en la que se encuentran. El tipo de red que vamos a estudiar en esta memoria son las redes hacia delante. En el aprendizaje de una red neuronal se distinguen dos fases; la fase de aprendizaje o entrenamiento y la fase de validación. Dentro de la fase de aprendizaje vamos a estudiar el aprendizaje de tipo supervisado, en el que se presenta a la red un conjunto de patrones de entrada junto con la salida deseada. En el capítulo 2, estudiamos el Perceptrón, concepto que fue introducido en 1958 por Frank Rosenblantt y que se trata de un modelo simple de neurona basado en el modelo de McCulloch y Pitts y en una regla de aprendizaje que consiste en la detección y corrección del error que se produce cuando la salida de la unidad no coincide conla salida deseada. Una de las características más importantes de este modelo es su capacidad de aprender a reconocer patrones. Es el modelo más sencillo de redes neuronales artificiales. Utiliza la función signo como función de activación. En 1960 surgió otro modelo de aprendizaje llamado Adaline, similar al Perceptrón simple pero utiliza como función de activación la función identidad y en este caso la salida es continua. La regla de aprendizaje que utiliza es la regla Delta, que trata de determinar los pesos sinápticos de manera que se minimice la función de error cuadrático. Para ello utiliza la regla del descenso del gradiente, es decir, en la misma dirección pero sentido opuesto al gradiente. En el capítulo 3, se introduce el Perceptrón multicapa, que surge de la incapacidad del Perceptrón simple de implementar la función lógica XOR.  Minsky y Papert demostraron que este problema podía ser resuelto introduciendo capas ocultas entre la capa de entrada y la de salida. En 1986 se abrió un nuevo panorama en el campo de la redes neuronales con el redescubrimiento del algoritmo de retropropagación. Se trata de un método eficiente para el entrenamiento de un Perceptrón multicapa. La regla de aprendizaje supervisado que se sigue para la determinación de los pesos sinápticos es muy similar que la del Adaline, pero ahora tenemos una capa de entrada compuesta por N neuronas de entrada, una número determinado de capas ocultas y M neuronas en la capa de salida. En esta memoria vamos a estudiar las redes que tienen una única capa oculta formada por L neuronas. Ahora la función de activación es una función diferenciable y no decreciente. Consideramos la función logística y la tangente hiperbólica. En el capítulo 4, vemos una aplicación de estos modelos en R. Primero comprobamos que el Perceptrón simple es capaz de implementar las funciones lógicas AND y OR y no la XOR, pero observamos como este problema se soluciona cuando introducimos una capa oculta de neuronas ya que entonces si es linealmente separable. Además, aplicamos estos algoritmos al caso práctico de concesión de créditos, con los datos de un banco alemán, ya que cuando un banco recibe una solicitud de préstamo, según el perfil del solicitante, tiene que tomar una decisión sobre si continuar con la aprobación del préstamo o no, teniendo en cuenta los riesgos que supone cada decisión y el objetivo del análisis es minimizar las pérdidas desde la perspectiva del banco. Realizamos un primer modelo de regresión logística y observamos que los resultados mejoran cuando añadimos una capa oculta. Estudiamos el número óptimo de neuronas que debe haber en esta capa para obtener un buen modelo de predicción y analizamos los resultados.</dc:description>
  <dc:format>pdf</dc:format>
  <dc:language>spa</dc:language>
  <dc:type>info:eu-repo/semantics/bachelorThesis</dc:type>
  <dc:rights>by-nc-sa</dc:rights>
  <dc:rights>http://creativecommons.org/licenses/by-nc-sa/3.0/</dc:rights>
</oai_dc:dc>