<?xml version="1.0" encoding="UTF-8"?><oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
  <dc:creator>Cáncer Gil, Jorge</dc:creator>
  <dc:creator>López Pellicer, Francisco Javier</dc:creator>
  <dc:title>GeoCrawler: sistema de crawler web enfocado al descubrimiento de información geográfica</dc:title>
  <dc:identifier>http://zaguan.unizar.es/record/60513</dc:identifier>
  <dc:publisher>Universidad de Zaragoza</dc:publisher>
  <dc:date>2016</dc:date>
  <dc:description>La búsqueda de información en la web es uno de los aspectos clave en la actualidad. Gran cantidad de herramientas buscan información en la web con motivos muy diversos. Entre esos motivos esta la realización de motores de búsqueda generales o enfocados, búsqueda de imágenes, comparadores de información, etc. Para alguno de ellos se utilizan crawlers. Los crawlers son robots o arañas que se van propagando por páginas web a través de los enlaces de estas. Recopilan información, así como enlaces (links) de entrada y enlaces de salida. El caso de la información geográfica es distinto, no es factible la recopilación de datos geográficos publicados por administraciones públicas por medio de crawlers genéricos. En este trabajo se ha realizado un crawler enfocado a la búsqueda y recolección de información geográfica publicada bajo alguno de los estándares OGC  Para ello se han realizado modificaciones al crawler de código abierto Nutch. Estas modificaciones actúan en partes concretas del flujo de trabajo del crawler para modificar o añadir funcionalidad del mismo. En concreto se ha modificado el sistema de scoring para adaptarlo a un sistema de búsqueda enfocado basado en un algoritmo de búsqueda llamado Shark-Search. Se ha hecho uso de un tesauro con términos geográficos para ayudar al crawler a estimar la importancia de una página web.  Los documentos recuperados se guardan en el sistema de ficheros de la máquina en la cual se está ejecutando. Una vez se acabó el crawler se realizaron pruebas para recolectar información. Durante esas pruebas se detectaron problemas que fueron solucionados. También se realizaron ajustes para mejorar el rendimiento del crawler. Tras las modificaciones se obtuvo un resultado muy prometedor llegando a recuperar gran cantidad de documentos con los que se pueden realizar tareas de análisis de datos o big data.</dc:description>
  <dc:format>pdf</dc:format>
  <dc:language>spa</dc:language>
  <dc:type>info:eu-repo/semantics/bachelorThesis</dc:type>
  <dc:rights>by-nc-sa</dc:rights>
  <dc:rights>http://creativecommons.org/licenses/by-nc-sa/3.0/</dc:rights>
</oai_dc:dc>