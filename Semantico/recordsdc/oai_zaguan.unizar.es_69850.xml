<?xml version="1.0" encoding="UTF-8"?><oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
  <dc:creator>García Artero, Christian</dc:creator>
  <dc:creator>Murillo Arnal, Ana Cristina</dc:creator>
  <dc:creator>Mosteo Chagoyen, Alejandro Rafael</dc:creator>
  <dc:title>Navegación visual controlada desde unas gafas de realidad aumentada</dc:title>
  <dc:identifier>http://zaguan.unizar.es/record/69850</dc:identifier>
  <dc:publisher>Universidad de Zaragoza</dc:publisher>
  <dc:date>2018</dc:date>
  <dc:description>Este proyecto presenta el diseño y implementación de un prototipo para la monitorización de las visualizaciones de una cámara remota, colocada en una plataforma móvil, desde unas gafas de visualización de realidad virtual/aumentada. La realidad aumentada/virtual y la robótica tienen una gran variedad de posibilidades,sondostemasenplenodesarrollodurantelos últimos años y con un gran potencial para desarrollar. El objetivo general del proyecto es seleccionar y poner en marcha los distintos componentes hardware necesarios (gafas de realidad aumentada y sistema de detección de movimiento integrado, cámara remota, sistema para mover la cámara), así como el software para procesar los datos de cada uno de estos componentes, y conectarlos para conseguir un prototipo de monitorización/visualización aumentada. El hardware seleccionado para el prototipo ﬁnal ha sido OSVR HDK2 como gafas de realidad aumentada, GoPro Hero 3+ como cámara remota y Pan-tilt E46-17 como sistema para mover la cámara. Y las herramientas clave en el desarrollo del prototipo han sido: OSVR para el funcionamiento de las gafas de realidad aumentada, OpenCV para el procesado de las imágenes de la cámara remota y ROS para comunicación entre las gafas y el dispositivo para mover la cámara. La arquitectura del sistema propuesto e implementado tiene tres módulos principales: Módulo 1: Captura de las imágenes de la cámara y anotación automática, para lo que se ha implementado un receptor de imágenes en tiempo real y un detector de personas.  Módulo 2: Gestión de las imágenes para su proyección en las gafas. Evaluación de las distintas posibilidades disponibles e integración en el sistema.  Módulo 3: Sincronización del movimiento de la cámara con el soporte de la cámara. Evaluación e implementación de la comunicación entre las gafas y el soporte de la cámara.  Los tres módulos se han conseguido poner en marcha e implementar de manera satisfactoria. La integración de los mismos ha dado lugar a la prueba de concepto objetivo.  Este prototipo ha permitido analizar las componentes más críticas, en este caso la integración de las gafas de realidad aumentada, y ha resultado en un módulo que se utilizará como base en futuros proyectos de mayor amplitud.</dc:description>
  <dc:format>pdf</dc:format>
  <dc:language>spa</dc:language>
  <dc:type>info:eu-repo/semantics/bachelorThesis</dc:type>
  <dc:rights>by-nc-sa</dc:rights>
  <dc:rights>http://creativecommons.org/licenses/by-nc-sa/3.0/</dc:rights>
</oai_dc:dc>